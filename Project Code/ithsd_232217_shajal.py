# -*- coding: utf-8 -*-
"""ITHSD-232217-Shajal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IB-nmD7UPXC9P204tN4LxKVMuQEwXSP0

Predicting IT Hardware Service Times and
Technician Performance Using Machine Learning

Md. Afjal Hossan shajal

ID:232217

Batch:PMIT-(Pre-29)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import scipy as sc

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import precision_score,recall_score,f1_score
from sklearn.metrics import roc_curve, auc

df = pd.read_csv("/content/ITHSD-Final DataSet.csv")
df.shape

df.head()

from sklearn.impute import SimpleImputer

# Replace "out" with NaN
df["DeliveryDate"] = df["DeliveryDate"].replace("out", np.nan)

# Convert to datetime (errors='coerce' turns invalid into NaT)
df["DeliveryDate"] = pd.to_datetime(df["DeliveryDate"], errors="coerce", dayfirst=True)


df["ReceiveDate"] = pd.to_datetime(df["ReceiveDate"], errors="coerce", dayfirst=True)


# Convert datetime → ordinal numbers for imputation
df["DeliveryOrdinal"] = df["DeliveryDate"].map(lambda x: x.toordinal() if pd.notnull(x) else np.nan)

# Impute missing values with most_frequent
imputer = SimpleImputer(strategy="most_frequent")
df["DeliveryOrdinal"] = imputer.fit_transform(df[["DeliveryOrdinal"]])

# Convert back ordinal → datetime
df["DeliveryDate"] = pd.to_datetime(df["DeliveryOrdinal"].apply(lambda x: pd.Timestamp.fromordinal(int(x))))

# Drop helper column
df = df.drop(columns=["DeliveryOrdinal"])

print("✅ Missing values handled with MOST FREQUENT imputation successfully.")
print(df)

df.isna().sum()

# Extract day, month, and year from 'ReceiveDate'
df['ReceiveYear'] = df['ReceiveDate'].dt.year
df['ReceiveMonth'] = df['ReceiveDate'].dt.month
df['ReceiveDay'] = df['ReceiveDate'].dt.day

# Extract day, month, and year from 'DeliveryDate'
df['DeliveryYear'] = df['DeliveryDate'].dt.year
df['DeliveryMonth'] = df['DeliveryDate'].dt.month
df['DeliveryDay'] = df['DeliveryDate'].dt.day


# Drop the original 'ReceiveDate' and 'DeliveryDate' columns
df = df.drop(['ReceiveDate', 'DeliveryDate'], axis=1)

# Display the DataFrame with the new columns
display(df[['ReceiveYear', 'ReceiveMonth', 'ReceiveDay', 'DeliveryYear', 'DeliveryDay', 'DeliveryMonth']].head())

df['ReceiveDateTime'] = df.apply(
    lambda x: pd.to_datetime(f"{x.ReceiveYear}-{x.ReceiveMonth}-{x.ReceiveDay}"), axis=1
)

df['DeliveryDateTime'] = df.apply(
    lambda x: pd.to_datetime(f"{x.DeliveryYear}-{x.DeliveryMonth}-{x.DeliveryDay}"), axis=1
)

try:
    # Combine year, month, and day into datetime objects using a dictionary
    df['ReceiveDateTime'] = pd.to_datetime({'year': df['ReceiveYear'],
                                             'month': df['ReceiveMonth'],
                                             'day': df['ReceiveDay']})

    df['DeliveryDateTime'] = pd.to_datetime({'year': df['DeliveryYear'],
                                              'month': df['DeliveryMonth'],
                                              'day': df['DeliveryDay']})

    # Calculate the difference in days
    df['DeliveredBy (days)'] = (df['DeliveryDateTime'] - df['ReceiveDateTime']).dt.days

    # Drop the temporary datetime columns
    df = df.drop(['ReceiveDateTime', 'DeliveryDateTime'], axis=1)

    # Display the DataFrame with the new column
    display(df[['ReceiveDay', 'ReceiveMonth', 'ReceiveYear', 'DeliveryDay', 'DeliveryMonth', 'DeliveryYear', 'DeliveredBy (days)']].head())

except ValueError as e:
    print(f"ValueError: {e}")
    print("Inspecting data for potential issues:")
    # Check for non-integer values in date columns
    print("Data types of date columns:")
    print(df[['ReceiveYear', 'ReceiveMonth', 'ReceiveDay', 'DeliveryYear', 'DeliveryMonth', 'DeliveryDay']].dtypes)

    # Check for any potential out-of-bounds date values (e.g., month > 12)
    invalid_receive_dates = df[~pd.to_datetime({'year': df['ReceiveYear'],
                                                'month': df['ReceiveMonth'],
                                                'day': df['ReceiveDay']}, errors='coerce').notna()]
    if not invalid_receive_dates.empty:
        print("\nRows with invalid Receive Dates:")
        display(invalid_receive_dates[['ReceiveYear', 'ReceiveMonth', 'ReceiveDay']])

    invalid_delivery_dates = df[~pd.to_datetime({'year': df['DeliveryYear'],
                                                 'month': df['DeliveryMonth'],
                                                 'day': df['DeliveryDay']}, errors='coerce').notna()]
    if not invalid_delivery_dates.empty:
        print("\nRows with invalid Delivery Dates:")
        display(invalid_delivery_dates[['DeliveryYear', 'DeliveryMonth', 'DeliveryYear']])

except Exception as e:
    print(f"An unexpected error occurred: {e}")

from sklearn.preprocessing import LabelEncoder

# List of columns to label encode
columns_to_encode = ['UI', 'Location', 'ProductModel', 'Problem', 'Solution', 'Requisition', 'SolvedBy', 'Speciality']

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to each specified column
for col in columns_to_encode:
    df[col + '_E'] = label_encoder.fit_transform(df[col])

# Display the updated DataFrame with the new encoded columns (showing original and encoded for the first few rows)
display(df[['UI', 'UI_E', 'Location', 'Location_E', 'ProductModel', 'ProductModel_E', 'Problem', 'Problem_E', 'Solution', 'Solution_E', 'Requisition', 'Requisition_E', 'SolvedBy', 'SolvedBy_E', 'Speciality', 'Speciality_E']].head())

df.columns

X_REG = df.drop(['DeliveredBy (days)', 'RgSerialNO', 'UI', 'Location', 'ProductModel', 'SolvedBy_E', 'Problem', 'Solution', 'Requisition', 'SolvedBy', 'Speciality', 'UI_E', 'Location_E',
       'ProductModel_E', 'Problem_E', 'Solution_E', 'Requisition_E', 'SolvedBy_E', 'Speciality_E'], axis=1)
y_REG = df[['DeliveredBy (days)']]

y_REG.head()

X_REG.head()

X_CLS = df.drop(['DeliveredBy (days)', 'RgSerialNO', 'UI', 'Location', 'ProductModel', 'SolvedBy_E', 'ReceiveYear', 'ReceiveMonth',
       'ReceiveDay', 'DeliveryYear', 'DeliveryMonth', 'DeliveryDay', 'Problem', 'Solution', 'Requisition', 'SolvedBy', 'Speciality'], axis=1)
y_CLS = df[['SolvedBy_E']]

y_CLS.head()

X_CLS.head()

from sklearn.preprocessing import LabelEncoder

# List of columns to label encode
columns_to_encode = ['UI', 'Location', 'ProductModel', 'Problem', 'Solution', 'Requisition', 'SolvedBy', 'Speciality']

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to each specified column
for col in columns_to_encode:
    print(f"\nMapping for column: {col}")
    le = LabelEncoder()
    le.fit(df[col])
    mapping_df = pd.DataFrame({'Original Value': le.classes_, 'Encoded Value': le.transform(le.classes_)})
    display(mapping_df)

df['SolvedBy_E'].value_counts()

df['SolvedBy_E'].value_counts().plot(kind='bar', figsize=(8, 5), color=['green','Blue','red','Brown']);
plt.title('Total SolvedBy')
plt.xticks([0,1,2,3], ["0=Moshiur", "2=Shajal", "1=Sarju", "3=Shuvo"], rotation=0)
plt.xlabel("SolvedBy")
plt.ylabel('Count')
plt.xticks(rotation=0) #;

ct=pd.crosstab(df.UI_E,df.SolvedBy_E)
print (ct)
pd.crosstab(df['UI_E'],df.SolvedBy_E).plot(kind='bar',
                              figsize=(8,5),
                              color=['green','red','Blue','Brown'])
# Correlation chart
plt.title('Correlation of UI Vs SolvedBy')
plt.xlabel('UI="[0=Female, 1=Male]"')
plt.ylabel('Counts')
plt.legend(['Moshiur','Sarju','Shajal','Shuvo']);
plt.xticks(rotation=0);
plt.show()

ct=pd.crosstab(df.Location_E,df.SolvedBy_E)
print (ct)
pd.crosstab(df['Location_E'],df.SolvedBy_E).plot(kind='bar',
                              figsize=(10,6),
                              color=['green','red','Blue','Brown'])
# Correlatione chart
plt.title('Correlation of Location Vs SolvedBy')
plt.xlabel('Location')
plt.ylabel('Counts')
plt.legend(['Moshiur','Sarju','Shajal','Shuvo']);
plt.xticks(rotation=0);
plt.show()

ct=pd.crosstab(df.ProductModel_E,df.SolvedBy_E)
print (ct)
pd.crosstab(df['ProductModel_E'],df.SolvedBy_E).plot(kind='bar',
                              figsize=(10,6),
                              color=['green','red','Blue','Brown'])
# Correlation chart
plt.title('Correlation of ProductModel Vs SolvedBy')
plt.xlabel('ProductModel')
plt.ylabel('Counts')
plt.legend(['Moshiur','Sarju','Shajal','Shuvo']);
plt.xticks(rotation=0);
plt.show()

ct=pd.crosstab(df.Problem_E,df.SolvedBy_E)
print (ct)
pd.crosstab(df['Problem_E'],df.SolvedBy_E).plot(kind='bar',
                              figsize=(10,6),
                              color=['green','red','Blue','Brown'])
# Correlation of the chart
plt.title('Correlation of Problem Vs SolvedBy')
plt.xlabel('Problem')
plt.ylabel('Counts')
plt.legend(['Moshiur','Sarju','Shajal','Shuvo']);
plt.xticks(rotation=0);
plt.show()

ct=pd.crosstab(df.Solution_E,df.SolvedBy_E)
print (ct)
pd.crosstab(df['Solution_E'],df.SolvedBy_E).plot(kind='bar',
                              figsize=(10,6),
                              color=['green','red','Blue','Brown'])
# Correlation of the chart
plt.title('Correlation of Solution Vs SolvedBy')
plt.xlabel('Solution')
plt.ylabel('Counts')
plt.legend(['Moshiur','Sarju','Shajal','Shuvo']);
plt.xticks(rotation=0);
plt.show()

ct=pd.crosstab(df.Requisition_E,df.SolvedBy_E)
print (ct)
pd.crosstab(df['Requisition_E'],df.SolvedBy_E).plot(kind='bar',
                              figsize=(10,6),
                              color=['green','red','Blue','Brown'])
# Correlation of the chart
plt.title('Correlation of Requisition Vs SolvedBy')
plt.xlabel('Requisition')
plt.ylabel('Counts')
plt.legend(['Moshiur','Sarju','Shajal','Shuvo']);
plt.xticks(rotation=0);
plt.show()

ct=pd.crosstab(df.Speciality_E,df.SolvedBy_E)
print (ct)
pd.crosstab(df['Speciality_E'],df.SolvedBy_E).plot(kind='bar',
                              figsize=(10,6),
                              color=['green','red','Blue','Brown'])
# Correlation chart
plt.title('Correlation of Speciality Vs SolvedBy')
plt.xlabel('Speciality')
plt.ylabel('Counts')
plt.legend(['Moshiur','Sarju','Shajal','Shuvo']);
plt.xticks(rotation=0);
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=np.number)

# Calculate the correlation matrix
correlation_matrix = numeric_df.corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix')
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_REG, y_REG, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train.values.ravel())

# Make predictions
y_pred = rf_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

import joblib

# Save the model
joblib.dump(rf_regressor, 'rf_regressor_checkpoint.pkl')

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_REG, y_REG, test_size=0.2, random_state=42)

# Initialize and train the Gradient Boosting Regressor
gbr_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr_regressor.fit(X_train, y_train.values.ravel())

# Make predictions
y_pred_gbr = gbr_regressor.predict(X_test)

# Evaluate the model
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
rmse_gbr = np.sqrt(mse_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"Mean Squared Error (MSE): {mse_gbr:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_gbr:.2f}")
print(f"R-squared (R2): {r2_gbr:.2f}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Split data into training and testing sets
X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_CLS, y_CLS, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_cls, y_train_cls.values.ravel())

# Make predictions
y_pred_cls = rf_classifier.predict(X_test_cls)

# Evaluate the model
accuracy = accuracy_score(y_test_cls, y_pred_cls)
report = classification_report(y_test_cls, y_pred_cls)
confusion = confusion_matrix(y_test_cls, y_pred_cls)

print(f"Random Forest Classifier - Accuracy: {accuracy:.2f}")
print("\nRandom Forest Classifier - Classification Report:")
print(report)
print("\nRandom Forest Classifier - Confusion Matrix:")
print(confusion)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# y_test_cls and y_pred_cls are already defined from the previous cell
y_pred = y_pred_cls
y_test = y_test_cls

cm = confusion_matrix(y_test, y_pred)

# Get unique labels from the original target variable (y_CLS)
labels = np.unique(y_CLS)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)

disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix of RandomForestClassifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.grid(False)
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import numpy as np

# y_test_cls and rf_classifier are available from previous cells

# Binarize the output
y_test_bin = label_binarize(y_test_cls, classes=np.unique(y_CLS))
n_classes = y_test_bin.shape[1]

# Get predicted probabilities for each class
y_score_rf = rf_classifier.predict_proba(X_test_cls)

# Compute ROC curve and ROC area for each class
fpr_rf = dict()
tpr_rf = dict()
roc_auc_rf = dict()
for i in range(n_classes):
    fpr_rf[i], tpr_rf[i], _ = roc_curve(y_test_bin[:, i], y_score_rf[:, i])
    roc_auc_rf[i] = auc(fpr_rf[i], tpr_rf[i])

# Plot ROC curves
plt.figure(figsize=(8, 6))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr_rf[i], tpr_rf[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc_rf[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for RandomForestClassifier')
plt.legend(loc="lower right")
plt.show()

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Split data into training and testing sets ( X_CLS and y_CLS are defined)
X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_CLS, y_CLS, test_size=0.2, random_state=42)

# Initialize and train the XGBClassifier
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_CLS)), random_state=42)
xgb_classifier.fit(X_train_cls, y_train_cls)

# Make predictions
y_pred_xgb = xgb_classifier.predict(X_test_cls)

# Evaluate the model
accuracy_xgb = accuracy_score(y_test_cls, y_pred_xgb)
report_xgb = classification_report(y_test_cls, y_pred_xgb)
confusion_xgb = confusion_matrix(y_test_cls, y_pred_xgb)

print(f"XGBoost Classifier - Accuracy: {accuracy_xgb:.2f}")
print("\nXGBoost Classifier - Classification Report:")
print(report_xgb)
print("\nXGBoost Classifier - Confusion Matrix:")
print(confusion_xgb)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

#  y_test_cls and y_pred_xgb are available from previous cells
cm_xgb = confusion_matrix(y_test_cls, y_pred_xgb)

# Get unique labels from the original target variable (y_CLS)
labels = np.unique(y_CLS)

disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=labels)

disp_xgb.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix of XGBoost Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.grid(False)
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import numpy as np

# y_test_cls and xgb_classifier are available from previous cells

# Binarize the output
y_test_bin = label_binarize(y_test_cls, classes=np.unique(y_CLS))
n_classes = y_test_bin.shape[1]

# Get predicted probabilities for each class
y_score_xgb = xgb_classifier.predict_proba(X_test_cls)

# Compute ROC curve and ROC area for each class
fpr_xgb = dict()
tpr_xgb = dict()
roc_auc_xgb = dict()
for i in range(n_classes):
    fpr_xgb[i], tpr_xgb[i], _ = roc_curve(y_test_bin[:, i], y_score_xgb[:, i])
    roc_auc_xgb[i] = auc(fpr_xgb[i], tpr_xgb[i])

# Plot ROC curves
plt.figure(figsize=(8, 6))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr_xgb[i], tpr_xgb[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc_xgb[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for XGBoost Classifier')
plt.legend(loc="lower right")
plt.show()

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Split data into training and testing sets ( X_CLS and y_CLS are defined)
X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_CLS, y_CLS, test_size=0.2, random_state=42)

# Initialize and train the LGBMClassifier
lgbm_classifier = lgb.LGBMClassifier(objective='multiclass', num_class=len(np.unique(y_CLS)), random_state=42)
lgbm_classifier.fit(X_train_cls, y_train_cls)

# Make predictions
y_pred_lgbm = lgbm_classifier.predict(X_test_cls)

# Evaluate the model
accuracy_lgbm = accuracy_score(y_test_cls, y_pred_lgbm)
report_lgbm = classification_report(y_test_cls, y_pred_lgbm)
confusion_lgbm = confusion_matrix(y_test_cls, y_pred_lgbm)

print(f"LGBM Classifier - Accuracy: {accuracy_lgbm:.2f}")
print("\nLGBM Classifier - Classification Report:")
print(report_lgbm)
print("\nLGBM Classifier - Confusion Matrix:")
print(confusion_lgbm)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# y_test_cls and y_pred_lgbm are available from previous cells
cm_lgbm = confusion_matrix(y_test_cls, y_pred_lgbm)

# Get unique labels from the original target variable (y_CLS)
labels = np.unique(y_CLS)

disp_lgbm = ConfusionMatrixDisplay(confusion_matrix=cm_lgbm, display_labels=labels)

disp_lgbm.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix of LightGBM Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.grid(False)
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import numpy as np

# y_test_cls and lgbm_classifier are available from previous cells

# Binarize the output
y_test_bin = label_binarize(y_test_cls, classes=np.unique(y_CLS))
n_classes = y_test_bin.shape[1]

# Get predicted probabilities for each class
y_score_lgbm = lgbm_classifier.predict_proba(X_test_cls)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score_lgbm[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(8, 6))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for LightGBM Classifier')
plt.legend(loc="lower right")
plt.show()

import joblib

# Save the model
joblib.dump(lgbm_classifier, 'lgbm_classifier_checkpoint.pkl')

lgbm_classifier_loaded = joblib.load('lgbm_classifier_checkpoint.pkl')

import matplotlib.pyplot as plt
import numpy as np

# accuracy, accuracy_xgb, and accuracy_lgbm are available from previous cells

models = ['Random Forest', 'XGBoost', 'LightGBM']
accuracies = [accuracy, accuracy_xgb, accuracy_lgbm]

plt.figure(figsize=(8, 6))
plt.bar(models, accuracies, color=['blue', 'green', 'red'])
plt.ylim(0, 1)  # Accuracy is between 0 and 1
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison of Classification Models')
plt.show()

import numpy as np
import joblib
import pandas as pd

# Load the LightGBM Classifier model
lgbm_classifier_loaded = joblib.load('lgbm_classifier_checkpoint.pkl')

# Load the Random Forest Regressor model
rf_regressor_loaded = joblib.load('rf_regressor_checkpoint.pkl')

# Define the mapping for SolvedBy
solved_by_mapping = {
    0: 'Moshiur',
    1: 'Sarju',
    2: 'Shajal',
    3: 'Shuvo'
}


new_data_cls = np.array([[1, 7, 12, 9, 0, 1, 0]])

new_data_reg = np.array([[2025, 7, 1, 2025, 7, 5]])


# Make the classification prediction using the loaded LightGBM model
prediction_encoded_cls = lgbm_classifier_loaded.predict(new_data_cls.reshape(1, -1))
predicted_solver_encoded = prediction_encoded_cls[0]
predicted_solver_name = solved_by_mapping.get(predicted_solver_encoded, "Unknown")

print(f"For the given input data:")
print(f"- Predicted Solver (Classification): {predicted_solver_name}")

# it only includes date components.
prediction_reg = rf_regressor_loaded.predict(new_data_reg.reshape(1, -1))
predicted_delivered_by_days = prediction_reg[0]

print(f"- Predicted Delivered By (days) (Regression): {predicted_delivered_by_days:.2f} days")